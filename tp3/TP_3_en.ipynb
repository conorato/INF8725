{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:\n",
    "\n",
    "1) Last Name, First Name, Student Number:\n",
    "\n",
    "2) Last Name, First Name, Student Number:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's goal is to familiar yourself with the concept of segmentation and handling of colors images. To do so, the objective of this laboratory is to transform a real picture into a hand-drawn equivalent. Two main steps are invoved in the operation: In the first part, we will focus on edge detection (as a proxy for the lines of our drawing). The second part will be focused on smartly reducing the number of colors in the image.\n",
    "\n",
    "\n",
    "**Due date**: \n",
    "\n",
    "Assignment due date is for Monday December 7th, at 11h55 PM. A 3 points penalty per day will be applied in case of delay.\n",
    "\n",
    "**Submission files** :\n",
    "\n",
    "All code must be contained in the present template, as well as the answers to the questions (either commented within the code or with special Markdown/text cells).\n",
    "Please follow the order of the subject.\n",
    "Commenting the code is important and the overall clarity of the work will be taken in account. Make sure that every variable is clearly understandable and every figure readable.\n",
    "\n",
    "You will also have to submit a static **HTML** version of this notebook *File->Download as...->HTML*. Put all your files (ipynb, html, eventually others externals ones) in a single **.zip** archive, named after your student number (StudentNb1_StudentNb2.zip).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports et utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'gray' # Default color map, do not change it\n",
    "import matplotlib\n",
    "import scipy.signal\n",
    "matplotlib.rcParams['figure.figsize'] = (25.0, 10.0) # Default figure siz, change it only if relevant.\n",
    "from scipy.cluster.vq import kmeans\n",
    "\n",
    "def imshow(img, title=None, ax=None, is_bgr=False, cmap='gray'):\n",
    "    \"\"\"\n",
    "    param img: image to display (either hxwx3 or hxw)\n",
    "    param title: Figure's title\n",
    "    param ax: Axe on which the image will be shown. If none is given, a new axe will be created.\n",
    "    param is_bgr: If true, will convert the image from BGR to RGB\n",
    "    param cmap: Color map.\n",
    "    \"\"\"\n",
    "    show=False\n",
    "    plt.axis('off')\n",
    "    if ax is None:\n",
    "        show=True\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        ax = plt\n",
    "    else:\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "            ax.set_axis_off()\n",
    "    \n",
    "    if img.ndim==2:\n",
    "        ax.imshow(img, cmap='gray')\n",
    "    else:\n",
    "        if is_bgr:\n",
    "            img = img[:,:,::-1].copy()\n",
    "        ax.imshow(img)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def f2int(img):\n",
    "    return (img*255).astype(np.uint8)\n",
    "\n",
    "def int2f(img):\n",
    "    return img.astype(np.float32)/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Drawing implies lines. The first part of the lab is dedicated to edge detection in the image. To display the image, you are free to use the function **imshow** given in the previous cell.\n",
    "\n",
    "## Question 1\n",
    "\n",
    "\n",
    "\n",
    "Load the the **chaton.png** (\"chaton\"=kitten), convert it into float and display it.\n",
    "\n",
    "\n",
    "\n",
    "Make sure that the colors are correctly rendered (as in your viewer) and that the pixels' values are inside the range [0,1].\n",
    "\n",
    "> **Warning** If you use **openCV** (cv2), the default channels' order is <span style=\"color:blue\">b</span>, <span style=\"color:green\">g</span>, <span style=\"color:red\">r</span> instead of the conventional <span style=\"color:red\">r</span>, <span style=\"color:green\">g</span>, <span style=\"color:blue\">b</span>.\n",
    "\n",
    "> In most of this lab, we will work  with image of type **np.float32**. But some functions expect images in the range [0, 255] with uint8 dtype. To make it easier for you, we provide two functions **f2int** and **int2f** that'll let you switch between both types (see above cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "The lightness of an image is defined by:\n",
    "$$\n",
    "c = \\frac{1}{2}(\\max(r, g, b) + \\min(r, g, b))\n",
    "$$\n",
    "\n",
    "Compute and display the lightness of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "We will now focus on edge detection. Most of the methods you've seen during the course are very sensitive to noise. In our case, the kitten's fur can be assimilated to a pepper and salt noise. What type of filter should you use to attenuate it? Filter the lightness map witht this filter using a window size of 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Warning** For the next questions, we will compare different edge detections methods. For each method, you'll have to use the filtered lightness map from the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "Compute and display on two adjacents graphs (subplots):\n",
    "- The image of the horizontal gradient\n",
    "- The image of the vertical gradient\n",
    "\n",
    "On a third graph (new figure), display the magnitude of the gradient\n",
    "\n",
    "> To compute the gradient, you'll have to define yourself the convolution mask. For the convolution itself,you can use the function **scipy.signal.convolve2d**:\n",
    "```python\n",
    "convolved = scipy.signal.convolve2d(img, mask, mode='same')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Redo the same thing, but this time by pre-filtering the input lightness with a Gaussien  filter with a standard deviation $\\sigma=1$. You'll have to code yourself the gaussian mask (You can not use cv2.GaussianBlur). Justify the size of the mask (relatively to $\\sigma$).\n",
    "What happens when we vary $\\sigma$?\n",
    "\n",
    "> As a reminder, a  2D Gaussian is defined by \n",
    "$$G(x, y)=A e^{\\frac{-(x²+y²)}{2\\sigma²}}$$\n",
    "where $A$ is a constant such that the area under the Gaussian is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "In order to extract the edges from the magnitude image, we will use Otsu's method. You'll have to implement it.\n",
    "Complete the following function:\n",
    "```python\n",
    "def otsu_thresholding(img):\n",
    "    assert img.dtype==np.uint8, \"The image image must have a uint8 type\"\n",
    "    thresholds = np.unique(img)\n",
    "    hist, bins = np.histogram(img, np.arange(256))\n",
    "    bins = bins[:-1]\n",
    "    hist = hist/img.size\n",
    "    nu = 0\n",
    "    for k in thresholds:\n",
    "        hist_c1  = hist[:k] # Histogram of cluster 1 \n",
    "        hist_c2  = hist[k:] # Histogram of cluster 2\n",
    "        P1_k = ... # Probability to belong to cluster 1\n",
    "        P2_k = ... # To complete\n",
    "        if P1_k==0:\n",
    "            continue\n",
    "        if P2_k==0:\n",
    "            break\n",
    "        m1_k =  np.sum(bins[:k]*hist_c1) / P1_k.astype(np.float32) # Mean on cluster 1\n",
    "        m2_k =  ... # To complete\n",
    "        var_interclasse = ... # To complete\n",
    "        if var_interclasse>nu:\n",
    "            threshold = k\n",
    "            nu = var_interclasse\n",
    "    thresholded = ... # To complete\n",
    "    return thresholded\n",
    "```\n",
    "that takes a monochromatic image as a parameter and automatically threshold it using Otsu's method. \n",
    "\n",
    "> Careful, for Otsu's method, the image must be of type uint8.\n",
    "\n",
    "Compare Otsu's thresholding on the magnitude with and without the Gaussian prefiltering. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Let's now try edge detection using Marr-Hildreth's approach. A quick reminder of the method:\n",
    "* The image is filtered with a gaussian (parametrized by $\\sigma$)\n",
    "* The Laplacian is computed\n",
    "* We search the locations of the zero croissing of the Laplacian. We only keep the ones with an absolute difference greather than a given threshold $t<1$\n",
    "\n",
    "\n",
    "> The code  for this last step is given in the function below. It expects the Laplacian as an input as well as a treshold.\n",
    "\n",
    "\n",
    "Display the edges for $\\sigma=5$ and a threshold equal to 0.005. For the computation of the Laplacian, you need to create yourself the corresponding filter mask.\n",
    "Test the edge detection with different values of $\\sigma$ and threhsold seuil and indicate the effect of both parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_crossing(img, threshold):\n",
    "    out = np.zeros_like(img)\n",
    "    for i in range(1, img.shape[0]-1):\n",
    "        for j in range(1, img.shape[1]-1):\n",
    "            ec = 0\n",
    "            if img[i-1, j]*img[i+1, j] < 0: # y\n",
    "                ec = max(ec, np.abs(img[i+1, j]-img[i-1, j]))\n",
    "            if img[i, j-1]*img[i, j+1] < 0: # x\n",
    "                ec = max(ec, np.abs(img[i, j+1]-img[i, j-1]))\n",
    "            if img[i-1, j-1]*img[i+1, j+1] < 0: # diag\n",
    "                ec = max(ec, np.abs(img[i+1, j+1]-img[i-1, j-1]))\n",
    "            if img[i+1, j-1]*img[i-1, j+1] < 0: # other diag\n",
    "                ec = max(ec, np.abs(img[i+1, j-1]-img[i-1, j+1]))\n",
    "            \n",
    "            out[i, j] = ec\n",
    "    \n",
    "    threshold = threshold*np.max(out)\n",
    "    return out>threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Finally, you'll try Canny's approach to edge detection. You are not asked to implement it yourself: you can use the function from **openCV** in the following way:\n",
    "\n",
    "```python\n",
    "edges_canny = cv2.Canny(clarte, min_hysteresis, max_hysteresis) # lightness must be in uint8\n",
    "```\n",
    "where min_hysteresis, max_hysteresis represent respectively the lower and upper hysteris thresholds.\n",
    "Using a upper threshold to 100, compared the effects of differents values for the lower threshold. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "Display on a 2x2 subplots the edges obtained with the 4 differents approaches. Comments the results of each method and their respective flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "This part work on colors images. Therefore, you should start from the very first image loaded in question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "Load the image **chaton.png**, filter with a median-filter of size 9, convert it into float and display it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A picture contains much more different colors than a drawing. We will therefore work on an operation called  **posterization** that consist in reducing the number of colors contained in an image. We can do so in differents ways.\n",
    "\n",
    "### Naive posterization\n",
    "\n",
    "In this approach, we recense all the N unique colors in the image as an array $T_1$ with shape $N\\times 3$. Then we keep only the first K most represented colors such that we obtain a new array $T_2$ with shape $K \\times 3$. \n",
    "\n",
    "For each pixel of the image, we replace the original color as its closest one in $T_2$. Proximity is defined by using the euclidean distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "Implement the function\n",
    "```python \n",
    "def recense_colors(img):\n",
    "    ...\n",
    "    return unique_colors, count \n",
    "\n",
    "```\n",
    "That returns an array $T1$ of unique colors as well as the occurence of each one in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "Complete the following function\n",
    "```python\n",
    "def euclidean_distance(col1, col2):\n",
    "    \"\"\"\n",
    "    This function takes two colors arrays and return the table of all the distances between them\n",
    "    :param col1: Array of shape Nx3\n",
    "    :param col2: Array of shape Mx3\n",
    "    :return d: Array of euclidean distances of shape NxM\n",
    "\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return d\n",
    "\n",
    "def naive_posterization(img, K=32):\n",
    "    h, w, c = img.shape\n",
    "    unique_colors, counts = recense_colors(img)\n",
    "    T2 = ... # To complete: we only keep the K most frequent colors\n",
    "    distances = euclidean_distance(T2,  img.reshape(-1, c))\n",
    "    indices_d_min = ... # For each pixels, we recover the value of the indice that will give us the new color in T2.\n",
    "    posterization = ...\n",
    "    return posterization.reshape(h, w, c)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "Test and display the result of the posterization with K=64. What do you deduce from the limitations of the naive posterization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "To correct the defaults from the naive posterization, we will use a more sophisticated approach to reduce the number of colors. Instead of only keeping the most frequent ones, we will regroup the colors within K clusters in a way that each cluster contains colors that are close to each others. Our new array $T2$ will be composed of the mean of each cluster. This fundamental algorithm in AI is called K-mean. \n",
    "\n",
    "No worries, you don't have to implement it! You can simply use the following function:\n",
    "```python\n",
    "from scipy.cluster.vq import kmeans\n",
    "T2 = kmeans(T1, K)[0] # K is the number of colors you want to keep and T1 the array of unique colors..\n",
    "```\n",
    "\n",
    "Implement the function\n",
    "```python\n",
    "def posterizations_kmeans(img, K=8):\n",
    "    ...\n",
    "```\n",
    "that uses kmeans to compute T2 and posterizes the image accordingly. Display the results for K=2,K=8 et K=64. Conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15\n",
    "\n",
    "In the previous two cases, we have posterized the image globally without considering the fact that some colors are more important than others. Nonetheless, on a drawing, there are much more different hues than saturations or values (if you take a pencil box, you usually have about ten different hues but barely  two or three saturation/values per hue). \n",
    "\n",
    "Let's mimic this by posterizing our channels independantly. Taking inspiration from previous questions, implement the function:\n",
    "```python\n",
    "def posterize_grayscale(canal, K=8):\n",
    "    ...\n",
    "```\n",
    "that posterizes a single channel using k_means approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16\n",
    "For now, we will use a color space very similar to the HSI (seen in class) which is the HSV (Hue/Saturation/Value). \n",
    "To convert a RGB into HSV et reciprocally, use the following functions:\n",
    "```python\n",
    "hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "```\n",
    "Display separately the three channels H, S and V and comment the results. In which range are each channel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17\n",
    "Posterize each channel independantly such that:\n",
    "- Hue contains 8 differents elements\n",
    "- Saturation only 3\n",
    "- Value only 3\n",
    "\n",
    "Then reconstruct the image in the RGB color space (display it).\n",
    "\n",
    "What is the thoerical maximum RGB colors that the image can contain under those constraints? And in practise, how many colors are there after this posterization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18\n",
    "Let's finish our drawing by adding edges! We will use the ones obtained in the first part.\n",
    "\n",
    "> To do so, you have many options: indexation (set to 0 the colors of the images in the edges), piecewise multiplication, bitwise_xor... You're free to chose the approach you like the most!\n",
    "\n",
    "\n",
    "Create four subplots and display the differents edges (gradients, canny, LoG...) in back noir on top of the posterized image from previous question.\n",
    "Which one seems the most convincing for our application?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19\n",
    "In reality, most lines of a drawing are rarely perfectly black; their colors may depend of their widths and the colors of the object they delimit.\n",
    "By taking in account these two considerations, propose a way to improve our drawing effect. You can combine as you want any of our previously obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question bonus\n",
    "\n",
    "We provide an extension of the last image, on which we add a final effect. Load the image **dessin_chaton.png** and display it.\n",
    "Can you identify the effect added? Propose (and test) a simple way to reproduce it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('py38')",
   "language": "python",
   "name": "python37764bitpy386272c69a2da84f6ab9703b5e5560986c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
